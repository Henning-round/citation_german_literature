{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e91e44",
   "metadata": {},
   "source": [
    "# Scrape works from hausarbeiten.de\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ec927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import csv\n",
    "from Homework import Homework\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4dd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a388d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping from 03.05.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2bbdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(works, name):\n",
    "    with open(name, \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(works, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32aa21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name, \"rb\") as fp:  \n",
    "        obj= pickle.load(fp) # Unpickling\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86eef123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    -returns a list containing some metadata about the works and a link to the specific work (with beautiful soup) of one page of the database\n",
    "    -as example a page listing the works looks like this: https://www.hausarbeiten.de/search?product=ebook&source_type=document&field=title%2Csubtitle%2Cdata&language_id=1&price_range_id=1000&sort=weight-desc%2Cdate-desc&display=100&page=0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def listscraping(soup, option): #option 1 equals hausarbeiten.de and diplomarbeiten24.de option 2 equals grin.de\n",
    "    \n",
    "    homeworks=[]\n",
    "    \n",
    "    objectlist = soup.find(\"ul\", class_=\"objectlist\") #find specific homework documents on page\n",
    "    documentlist = objectlist.find_all(\"li\", class_=\"objectlist-item document\")\n",
    "    \n",
    "    for docs in documentlist:\n",
    "        try:\n",
    "            link = docs.find(\"a\")[\"href\"]\n",
    "        except AttributeError:\n",
    "            link = \"none\"\n",
    "    \n",
    "        try:\n",
    "            if option == 1:\n",
    "                title = docs.find(\"div\", class_=\"heading1\").find(\"a\").text\n",
    "            elif option ==2:  \n",
    "                title = docs.find(\"h2\").text\n",
    "        except AttributeError:\n",
    "            title = \"none\"\n",
    "  \n",
    "        try:\n",
    "            author = docs.find(\"dl\", class_=\"metalist\").find(\"dd\", class_=\"author\").find(\"a\").text\n",
    "        except AttributeError:\n",
    "            author = \"anonymous\" #special name for non existing author\n",
    "   \n",
    "        try:\n",
    "            subject = docs.find(\"dl\", class_=\"metalist\").find(\"dd\", class_=\"subject\").find(\"a\").text\n",
    "        except AttributeError:\n",
    "            subject = \"none\"\n",
    "        \n",
    "        try:\n",
    "            category = docs.find(\"dl\", class_=\"metalist\").find(\"dd\", class_=\"category\").text\n",
    "        except AttributeError:\n",
    "            category = \"none\"\n",
    "      \n",
    "        try:\n",
    "            price = docs.find(\"dl\", class_=\"metalist\").find(\"dd\", class_=\"price\").text\n",
    "        except AttributeError:\n",
    "            price= \"none\"\n",
    "       \n",
    "\n",
    "        homeworks.append(Homework(link,title,author,subject,category,price))\n",
    "\n",
    "    return homeworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5379f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    -this function iterates over all results from the search in the database and calls the listscraping function\n",
    "    params:\n",
    "        num: is the number of pages to iterate trough\n",
    "        sec: is the amount of time to wait for a request\n",
    "        option: depends on the website to crawl for (hausarbeiten.de has a different html structure than grin.de)\n",
    "        url: is the url of the search results of the databases without a page number\n",
    "            e.g. for hausarbeiten.de: https://www.hausarbeiten.de/search?product=ebook&source_type=document&field=title%2Csubtitle%2Cdata&language_id=1&price_range_id=1000&sort=weight-desc%2Cdate-desc&display=100&page=\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def collecthomeworks(num, sec,option,url): #number of pages to request and waiting time between requests\n",
    "    homeworks =[]\n",
    "\n",
    "    for count in tqdm(range(num)): #max range of pages in Hausarbeiten.de is 173 (180 grin.de, 15 diplomarbeiten24.de)\n",
    "       \n",
    "        source = get_data(url.format(count = count))\n",
    "        soup = BeautifulSoup(source, \"lxml\")\n",
    "\n",
    "        homeworks += listscraping(soup, option)\n",
    "\n",
    "        time.sleep(sec)\n",
    "    return homeworks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b97f0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text(soup):\n",
    "    \"\"\"extraction of the text of a work\"\"\"\n",
    "    \n",
    "    try:\n",
    "        text_plain= soup.find(\"div\", class_=\"plain-preview\").text\n",
    "    except AttributeError:\n",
    "        text_plain = \"none\"\n",
    "    \n",
    "        \n",
    "    \n",
    "    return text_plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feaee30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(soup):\n",
    "    \"\"\"some extraction of metadata\"\"\"\n",
    "    \n",
    "    try:\n",
    "        pages_displayed = soup.find(\"div\", class_=\"page-numbers\").text\n",
    "        pages_displayed = re.sub(r\"[\\n\\t\\xa0]\", \"\", pages_displayed)\n",
    "    except AttributeError:\n",
    "        pages_displayed = \"none\"\n",
    "    \n",
    "    try:\n",
    "        grade = soup.find(\"dd\", class_=\"grade\").text\n",
    "    except AttributeError:\n",
    "        grade = \"none\"\n",
    "    \n",
    "    try:\n",
    "        pages = soup.find(\"dd\", class_=\"page_count\").text\n",
    "    except AttributeError:\n",
    "        pages = \"none\"\n",
    "        \n",
    "    try:\n",
    "        institution = soup.find(\"dd\", class_=\"institution\").text\n",
    "        institution = re.sub(r\"[\\n\\t\\xa0]\", \"\", institution)\n",
    "    except AttributeError:\n",
    "        institution = \"none\"\n",
    "        \n",
    "    try:\n",
    "        isbn = soup.find(\"dd\", class_=\"isbn\").text\n",
    "    except AttributeError:\n",
    "        isbn = \"none\"\n",
    "    \n",
    "    try:\n",
    "        tags = soup.find(\"dd\", class_=\"tags\").text\n",
    "        tags = re.sub(r\"[\\n]\", \" \", tags)\n",
    "    except AttributeError:\n",
    "        tags = \"none\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    return {\"grade\": grade, \"pages\": pages, \"institution\": institution, \"isbn\": isbn, \"tags\": tags, \"pages_displayed\": pages_displayed}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1fc0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"function for requesting the servers of grin\"\"\"\n",
    "\n",
    "def retry(func, retries=100):\n",
    "    def retry_wrapper(*args, **kwargs):\n",
    "        attempts=0\n",
    "        while attempts < retries:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(e)\n",
    "                time.sleep(30)\n",
    "                attempts += 1\n",
    "    return retry_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a0f9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry\n",
    "def get_data(url):\n",
    "    r = requests.get(url)\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc7666b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3134e41b614895bcf3ee17277b9426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = collecthomeworks(173,3,1,\"https://www.hausarbeiten.de/search?product=ebook&source_type=document&field=title%2Csubtitle%2Cdata&language_id=1&price_range_id=1000&sort=weight-desc%2Cdate-desc&display=100&page=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56968af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(lists, r\"C:\\Users\\Tim\\Desktop\\bachelorarbeit\\lists_for_scraping\\homeworks_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a4a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeworks = load_obj(r\"C:\\Users\\Tim\\Desktop\\bachelorarbeit\\lists_for_scraping\\grins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a60e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_data(homeworks, entry, name):\n",
    "    \"\"\"this function now extracts all exact information of a specific work\"\"\"\n",
    "    count = 0\n",
    "    num= int(entry / 500)\n",
    "    worklist = []\n",
    "    \n",
    "    filename = name + \"{num}\"\n",
    "    for work in tqdm(homeworks[entry:]):\n",
    "      \n",
    "        count+=1\n",
    "        source = get_data(work.link)\n",
    "        soup = BeautifulSoup(source, \"lxml\")\n",
    "        \n",
    "        if (work.price.lower() == \"kostenlos\") or (work.price.lower() == \"free\") :\n",
    "            work.text_html = extract_text(soup)\n",
    "        else:\n",
    "            work.text_html = \"none\"\n",
    "            \n",
    "        dic = extract_metadata(soup)\n",
    "\n",
    "        for key, value in dic.items():\n",
    "            setattr(work, key, value)\n",
    "\n",
    "        worklist.append(work)\n",
    "        \n",
    "        \n",
    "        if count % 500 == 0:\n",
    "            \n",
    "            save_obj(worklist, filename.format(num=num))\n",
    "            worklist = []\n",
    "            num +=1\n",
    "           \n",
    "      \n",
    "        \n",
    "        time.sleep(2)\n",
    "    save_obj(worklist, filename.format(num=num))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ed487fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42038ae926d4185af7a980a78a1a420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mextract_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomeworks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTim\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbachelorarbeit\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mobjects\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mextract_all_data\u001b[1;34m(homeworks, entry, name)\u001b[0m\n\u001b[0;32m     30\u001b[0m         worklist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     31\u001b[0m         num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m save_obj(worklist, filename\u001b[38;5;241m.\u001b[39mformat(num\u001b[38;5;241m=\u001b[39mnum))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### save the objects in folder obejcts\n",
    "extract_all_data(homeworks, 0, r\"C:\\Users\\Tim\\Desktop\\bachelorarbeit\\objects\\homeworks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5e6a4",
   "metadata": {},
   "source": [
    "# Complement works from grin.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb94327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(works):\n",
    "    link = [work.link.split(\"/\")[-1] for work in works]\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "198cbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeworks = load_obj(\"homeworks_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "grin_list = collecthomeworks(180,0,\"https://www.diplomarbeiten24.de/search?product=ebook&source_type=document&field=title%2Csubtitle%2Cdata&language_id=1&price_range_id=1000&sort=weight-desc%2Cdate-desc&display=100&page={count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"check if works on hausarbeiten.de and grin.de are different\"\"\"\n",
    "\n",
    "homework_links = get_link(homeworks)\n",
    "grins=[] ##here are the works which are not on hausarbeiten.de\n",
    "\n",
    "for work in grin_list:\n",
    "    if work.link.split(\"/\")[-1] not in homework_links:\n",
    "        grins.append(work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4d780573",
   "metadata": {},
   "outputs": [],
   "source": [
    "grins = load_obj(\"grins\") #these are all works which are on grin.de but not on hausarbeiten.de (total 575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "89ba6abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ab2cdde7894702b0d7c0d23b8920c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n"
     ]
    }
   ],
   "source": [
    "extract_all_data(grins, 0, \"objects_grin/grins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a616a5cf",
   "metadata": {},
   "source": [
    "# Works from diplomarbeit24.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "894d5617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af131f35da7344bf866df1074e7b923e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diploms = collecthomeworks(15,0,1,\"https://www.diplomarbeiten24.de/search?product=ebook&source_type=document&field=title%2Csubtitle%2Cdata&language_id=1&price_range_id=1000&sort=weight-desc%2Cdate-desc&display=100&page={count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "36675116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Günter Rodegast (Autor:in)',\n",
      " 'category': 'Forschungsarbeit, 2007',\n",
      " 'grade': '',\n",
      " 'institution': '',\n",
      " 'isbn': '',\n",
      " 'link': 'https://www.diplomarbeiten24.de/document/110637',\n",
      " 'pages': '',\n",
      " 'price': 'Kostenlos',\n",
      " 'subject': 'Geschichte Europa - Deutschland - Nationalsozialismus, II. '\n",
      "            'Weltkrieg',\n",
      " 'tags': '',\n",
      " 'text_html': '',\n",
      " 'title': 'Juden in der Region in und um Wittenberge'}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(diploms[1000\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "863c28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(diploms, \"diplom_list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88b6281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeworks = load_obj(r\"C:\\Users\\Tim\\Desktop\\bachelorarbeit\\lists_for_scraping\\homeworks_list\")\n",
    "grins= load_obj(r\"C:\\Users\\Tim\\Desktop\\bachelorarbeit\\lists_for_scraping\\grins\")\n",
    "h = get_links(homeworks)\n",
    "g = get_links(grins)\n",
    "diploms = load_obj(r\"C:\\Users\\Tim\\Desktop\\bachelorarbeit\\lists_for_scraping\\diplom_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0004b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for work in diploms:\n",
    "    if (work.link.split(\"/\")[-1] not in h) and (work.link.split(\"/\")[-1] not in g):\n",
    "        count +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee30b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
